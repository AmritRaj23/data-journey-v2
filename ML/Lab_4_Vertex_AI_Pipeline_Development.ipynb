{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6a6b2-43f7-449f-94a2-619e4daa8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d6471c-69ba-4987-933f-e0e2eac2b42a",
   "metadata": {},
   "source": [
    "# **Lab 4:** Vertex AI Pipeline Development\n",
    "This lab orchestrates the content Labs 2 & 3 into a vertex pipeline. In our pipeline we will use supported operators to: \n",
    "* **BigqueryCreateModelJobOp**: To train a logistic regression model \n",
    "* **BigqueryPredictModelJobOp**: To run our predictions and save the results to a BQ table\n",
    "* **BigqueryExportModelJobOp**, **importer_node**, **ModelUploadOp**: To export our BQML model in tensorflow format in GCS and upload to vertex model registry\n",
    "* **EndpointCreateOp**: To create our endpoint\n",
    "* **ModelDeployOp**: To deploy our registered model to our endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7758b-d973-4ad1-b2c5-923216123772",
   "metadata": {},
   "source": [
    "![overview](assets/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631f759-7352-4940-94be-97e605ad11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade \"kfp\" \\\n",
    "                         \"google-cloud-aiplatform\" \\\n",
    "                         \"google-cloud-storage\" \\\n",
    "                         \"google_cloud_pipeline_components\" \\\n",
    "                         \"google-cloud-bigquery\" --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa8a5d-9c97-47c7-b34c-392f8725a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"\"\n",
    "location     = \"us\"\n",
    "region       = \"us-central1\"\n",
    "team_name    = \"\" \n",
    "dataset_name = \"datathon_ds_{}\".format(team_name)\n",
    "bucket_name  = \"gs://{}_{}\".format(project_id,dataset_name)\n",
    "pipeline_root_path = bucket_name + '/pipelines/'\n",
    "model_artificat_path = bucket_name + '/vertex_pipelines_models'\n",
    "model_artificat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78164d6-35e0-4e77-b480-3798fd94e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e3633-603e-49ce-9dcb-db236f97564d",
   "metadata": {},
   "source": [
    "We need to restart the kernel to make sure we reference the newly installed libraries\n",
    "* **Kernel** -> **Restart Kernel and clear outputs**\n",
    "\n",
    "Execute the commands from the first cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d42428-b123-4a19-bc2d-899441cddaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.components import importer_node\n",
    "from kfp.v2.dsl import HTML, Artifact, Condition, Input, Output, component\n",
    "from google_cloud_pipeline_components.v1.bigquery import (\n",
    "    BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp, BigqueryExportModelJobOp,\n",
    "    BigqueryExplainForecastModelJobOp, BigqueryForecastModelJobOp,BigqueryMLConfusionMatrixJobOp, BigqueryPredictModelJobOp,\n",
    "    BigqueryMLArimaEvaluateJobOp, BigqueryQueryJobOp)\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp, ModelDeployOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4071464-7818-4bcc-a0d7-15c168158e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@dsl.pipeline(\n",
    "    name=\"bqml-pipeline\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "def pipeline(project_id: str):\n",
    "    # Learn more about operators used here -> https://cloud.google.com/vertex-ai/docs/pipelines/bigqueryml-component\n",
    "    model_create = BigqueryCreateModelJobOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        query=\"\"\"\n",
    "        CREATE OR REPLACE MODEL `<project-id>.<dataset-id>.vertex_pipeline_logistic_regression_baseline`\n",
    "        OPTIONS(MODEL_TYPE='LOGISTIC_REG',\n",
    "                INPUT_LABEL_COLS = ['churned'])\n",
    "        AS\n",
    "        SELECT * EXCEPT (user_pseudo_id)\n",
    "        FROM `<project-id>.<dataset-id>.cc_train_dataset`    \n",
    "        \"\"\"\n",
    "        ).set_display_name(\"Train logistic regression baseline\")\n",
    "    \n",
    "    _ = BigqueryPredictModelJobOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        model=model_create.outputs[\"model\"],\n",
    "        query_statement=f'''SELECT * EXCEPT(user_pseudo_id, churned) \n",
    "                            FROM\n",
    "                            `<project-id>.<dataset-id>.cc_eval_dataset`\n",
    "                            ''',\n",
    "        job_configuration_query={\n",
    "            \"destinationTable\": {\n",
    "                \"projectId\": \"<project-id>\",\n",
    "                \"datasetId\": \"<dataset-id>\", \n",
    "                \"tableId\": \"results_1\", # change table for every new run\n",
    "            }\n",
    "        },\n",
    "        ).set_display_name(\"Prediction on evaluation set\").after(model_create)\n",
    "    \n",
    "    bq_export = BigqueryExportModelJobOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        model=model_create.outputs[\"model\"],\n",
    "        model_destination_path=model_artificat_path,\n",
    "    ).set_display_name(\"Export BQ model to GCS\").after(model_create)\n",
    "    \n",
    "    import_unmanaged_model_task = importer_node.importer(\n",
    "        artifact_uri=model_artificat_path,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/tf-cpu.1-15:latest\",\n",
    "            },\n",
    "        },\n",
    "    ).after(bq_export)\n",
    "    \n",
    "    model_upload = ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=\"vertex_pipeline_model_logistic_regression\",\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    ).after(import_unmanaged_model_task)\n",
    "    \n",
    "    endpoint = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        display_name=\"vertex_pipeline_deployment\",\n",
    "    ).after(model_upload)\n",
    "    \n",
    "    _ = ModelDeployOp(\n",
    "        model=model_upload.outputs[\"model\"],\n",
    "        endpoint=endpoint.outputs[\"endpoint\"],\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        dedicated_resources_machine_type='n1-standard-2',\n",
    "        traffic_split={\"0\": 100},\n",
    "    ).set_display_name(\"Deploy to endpoint\").after(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159558d-93ae-4705-b8e8-e3c0db40309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='churn_prediction_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a94b0-6a23-4554-b7fe-3a281408df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"bqml-vertex-pipeline\",\n",
    "    template_path=\"churn_prediction_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values={\n",
    "        'project_id': project_id\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
